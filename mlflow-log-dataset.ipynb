{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3bb5177-9089-4aa2-ab57-6c6ce805a2ff",
   "metadata": {},
   "source": [
    "# Logging Datasets in Machine Learning Experiments with MLflow\n",
    "\n",
    "MLflow is widely recognized as a powerful tool for tracking machine learning (ML) experiments, enabling data scientists and ML experts to systematically log metrics, parameters, and models. However, its capabilities extend beyond these traditional use cases—MLflow can also be used to **track datasets**. Logging information about training and evaluation datasets is a critical step in enhancing the transparency and reproducibility of ML experiments. By capturing dataset details, such as versioning and splits, MLflow helps ensure that experiments can be accurately replicated and understood.\n",
    "\n",
    "In this post, I demonstrate how to use MLflow to track datasets alongside parameters, metrics, and other experiment artifacts. As an example, I use the [Germany Cars Dataset](https://www.kaggle.com/datasets/ander289386/cars-germany), a publicly available dataset on Kaggle, to build a model for predicting the prices of German cars.\n",
    "\n",
    "If you're interested in learning more about this dataset and how to clean and prepare it for modeling, you can check out my other [post](https://medium.com/@mohsenim/tracking-machine-learning-experiments-with-mlflow-and-dockerizing-trained-models-germany-car-price-e539303b6f97).\n",
    "\n",
    "## How to Track Datasets with MLflow\n",
    "\n",
    "MLflow provides several ways to log datasets, including:\n",
    "- **Logging dataset artifacts**: Save the dataset file itself (e.g., CSV, Parquet) as an artifact.\n",
    "- **Logging dataset splits and metadata**: Record information such as dataset name, source, and the dataset splits, i.e., training, validation, and test sets. This is the focus of this post.\n",
    "\n",
    "MLflow natively supports the following dataset types:\n",
    "\n",
    "- `mlflow.data.pandas_dataset.PandasDataset`\n",
    "- `mlflow.data.numpy_dataset.NumpyDataset`\n",
    "- `mlflow.data.spark_dataset.SparkDataset`\n",
    "- `mlflow.data.huggingface_dataset.HuggingFaceDataset`\n",
    "- `mlflow.data.tensorflow_dataset.TensorFlowDataset` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a98c6-ce15-459d-9a70-859d96b753ca",
   "metadata": {},
   "source": [
    "## Traning a Car Price Perdiction Model \n",
    "Let’s start by importing the necessary libraries for reading the dataset, training models, and tracking models and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2900853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn import preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afb8505-1f2f-4a9e-a355-c7a3637362be",
   "metadata": {},
   "source": [
    "## MLflow Server\n",
    "An MLflow tracking server is used to monitor our experiment and log parameters and metrics. We assumes the URI of the MLflow server to be `http://localhost:8080`. If you are using a different server or port, you can modify the URI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb3b877-3079-499a-bb9b-ed257ec41ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543dbf21-c963-47e6-9203-133c80b4ee74",
   "metadata": {},
   "source": [
    "## ML Model\n",
    "\n",
    "We define a function that creates a pipeline for preprocessing categorical variables and incorporates an [XGBoost](https://xgboost.readthedocs.io) model as our machine learning model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9a9c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xgb_model_pipeline(categorical_cols, max_depth, learning_rate):\n",
    "    \"\"\"\n",
    "    Build the pipline\n",
    "    \"\"\"\n",
    "    ordinal_encoder = preprocessing.OrdinalEncoder()\n",
    "    preprocess = ColumnTransformer(\n",
    "        [(\"Ordinal-Encoder\", ordinal_encoder, categorical_cols)],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    xgb_model = xgb.XGBRegressor(max_depth=max_depth, learning_rate=learning_rate)\n",
    "    pipeline = Pipeline([(\"preprocess\", preprocess), (\"xgb_model\", xgb_model)])\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e12ee3-e277-4791-84a9-e0da4c4c19b0",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n",
    "\n",
    "The following code reads the dataset into a pandas dataframe, which includes both categorical and numerical columns. We need to specify the categorical columns as the pipeline defined above should be aware of them to transform them before passing them to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "238f0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"./data/autoscout24-germany-dataset-cleaned.csv\"\n",
    "df = pd.read_csv(dataset_path)\n",
    "categorical_cols = [\"make\", \"model\", \"fuel\", \"gear\", \"offerType\"]\n",
    "numerical_cols = [\"mileage_log\", \"hp\", \"age\", \"price_log\"]\n",
    "\n",
    "cols = categorical_cols + numerical_cols\n",
    "data = df[cols]\n",
    "\n",
    "df_train, df_test = train_test_split(data, test_size=0.20, random_state=37)\n",
    "train_x = df_train.drop([\"price_log\"], axis=1)\n",
    "train_y = df_train[[\"price_log\"]]\n",
    "\n",
    "test_x = df_test.drop([\"price_log\"], axis=1)\n",
    "test_y = df_test[[\"price_log\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed75b2b1",
   "metadata": {},
   "source": [
    "## Train Model and Log Artifacts\n",
    "\n",
    "In the following code, we first set a name for our experiment that MLflow will use to group all artifacts related to this experiment in one place. \n",
    "In the function `train`, we define an MLflow dataset and set metadata such as `source`, `name`, and the target column, `price_log`. If you'd like to save the splits as well, you can do that. In the code for the train dataset, an MLflow dataset is also created.\n",
    "\n",
    "After setting the experiment and starting a run, we can save the datasets using `mlflow.log_input`. It is also possible to define a `context` for adding more information and showing where a dataset has been used. \n",
    "The code also logs two parameters of the model and the evaluation metric, which is the **mean squared error (MSE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3fbdf066",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name=\"german-car-price\"\n",
    "artifact_path=\"german_car_model\"\n",
    "\n",
    "max_depth=6\n",
    "learning_rate=0.1\n",
    "model = get_xgb_model_pipeline(categorical_cols, max_depth, learning_rate)\n",
    "\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    dataset = mlflow.data.from_pandas(df,\n",
    "                                      source=\"./data/autoscout24-germany-dataset.csv\", \n",
    "                                      name=\"Germany Car Dataset - Cleaned\", \n",
    "                                      targets=\"price_log\"\n",
    "                                      )\n",
    "\n",
    "    dataset_train = mlflow.data.from_pandas(df_train, \n",
    "                                            name=\"Training dataset\", \n",
    "                                            targets=\"price_log\"\n",
    "                                            )\n",
    "\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_input(dataset=dataset)\n",
    "        mlflow.log_input(dataset=dataset_train, context=\"training\")\n",
    "\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Evaluation\n",
    "        pred_y = model.predict(test_x)\n",
    "        eval_metric = mean_squared_error(test_y, pred_y)\n",
    "\n",
    "        # Log the parameters, metric and model\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "        mlflow.log_metric(\"MSE\", eval_metric)\n",
    "\n",
    "        signature = infer_signature(train_x[:1], train_y[:1])\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            artifact_path=artifact_path, \n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "    return eval_metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17756b58",
   "metadata": {},
   "source": [
    "Let’s run the code and see the results, which show 0.00374 as the MSE of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ab9c921-ee03-4961-b901-b8aebad10075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained! Mean squared error (MSE) of the model: 0.0037354609359017722\n"
     ]
    }
   ],
   "source": [
    "result = train()\n",
    "print(f\"Trained! Mean squared error (MSE) of the model: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0d13e",
   "metadata": {},
   "source": [
    "## A Cool Feature of MLFlow\n",
    "\n",
    "Before showing the results in the MLflow UI, I want to highlight a cool feature of MLflow. Instead of manually evaluating a model and logging a single metric, you can use `mlflow.evaluate` to perform the evaluation using several metrics based on the type of your ML task. If you allow MLflow to handle this, it will also automatically log the evaluation dataset. I’ve updated the code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e1d71cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/26 00:33:46 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2025/01/26 00:33:46 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2025/01/26 00:33:46 WARNING mlflow.models.evaluation.default_evaluator: Skip logging model explainability insights because the shap explainer None requires all feature values to be numeric, and each feature column must only contain scalar values.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>example_count</th>\n",
       "      <td>7656.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_error</th>\n",
       "      <td>0.045779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_squared_error</th>\n",
       "      <td>0.003735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>root_mean_squared_error</th>\n",
       "      <td>0.061118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sum_on_target</th>\n",
       "      <td>31035.706183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_on_target</th>\n",
       "      <td>4.053776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2_score</th>\n",
       "      <td>0.946087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max_error</th>\n",
       "      <td>0.442580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_absolute_percentage_error</th>\n",
       "      <td>0.011402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       value\n",
       "example_count                    7656.000000\n",
       "mean_absolute_error                 0.045779\n",
       "mean_squared_error                  0.003735\n",
       "root_mean_squared_error             0.061118\n",
       "sum_on_target                   31035.706183\n",
       "mean_on_target                      4.053776\n",
       "r2_score                            0.946087\n",
       "max_error                           0.442580\n",
       "mean_absolute_percentage_error      0.011402"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = get_xgb_model_pipeline(categorical_cols, max_depth, learning_rate)\n",
    "\n",
    "def train():\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    dataset = mlflow.data.from_pandas(df,\n",
    "                                      source=\"./data/autoscout24-germany-dataset.csv\", \n",
    "                                      name=\"Germany Car Dataset - Cleaned\", \n",
    "                                      targets=\"price_log\"\n",
    "                                      )\n",
    "\n",
    "    dataset_train = mlflow.data.from_pandas(df_train, \n",
    "                                            name=\"Training dataset\", \n",
    "                                            targets=\"price_log\"\n",
    "                                            )\n",
    "\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=experiment_name)\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_input(dataset=dataset)\n",
    "        mlflow.log_input(dataset=dataset_train, context=\"training\")\n",
    "\n",
    "        model.fit(train_x, train_y)\n",
    "\n",
    "        # Evaluation\n",
    "        pred_y = model.predict(test_x)\n",
    "        eval_metric = mean_squared_error(test_y, pred_y)\n",
    "\n",
    "        # Log the parameters, metric and model\n",
    "        mlflow.log_param(\"max_depth\", max_depth)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "\n",
    "        signature = infer_signature(train_x[:1], train_y[:1])\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            artifact_path=artifact_path, \n",
    "            signature=signature\n",
    "        )\n",
    "\n",
    "        df_test['prediction'] = pred_y\n",
    "        dataset_test = mlflow.data.from_pandas(df_test,\n",
    "                                            name=\"Test dataset\",\n",
    "                                            predictions=\"prediction\", \n",
    "                                            targets=\"price_log\"\n",
    "                                            )\n",
    "        result = mlflow.evaluate(data=dataset_test, predictions=None, model_type=\"regressor\")\n",
    "    return result\n",
    "\n",
    "result = train()\n",
    "\n",
    "metrics = pd.DataFrame.from_dict(result.metrics, orient='index', columns=['value'])\n",
    "display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b22bd30",
   "metadata": {},
   "source": [
    "The code produces the following output. As we can see, based on our task (regression), MLflow automatically calculated several relevant metrics for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f872ee3-2004-465c-81b1-71559a317d84",
   "metadata": {},
   "source": [
    "## Looking at Results in the MLflow UI\n",
    "\n",
    "Let’s take a look at the UI. In the following image, we can see that MLflow has logged three datasets for our experiment.\n",
    "\n",
    "![datasets](./fig/dataset_ui.png)\n",
    "\n",
    "The list of metrics can also be seen in the UI.\n",
    "\n",
    "![metrics_ui](./fig/metrics_ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49890563-e6ff-46e5-a639-652509c37aa2",
   "metadata": {},
   "source": [
    "👍 Thumbs up or 👏 clap if you liked this post!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
